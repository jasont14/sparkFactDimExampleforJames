# COMMAND ----------

import time
from datetime import datetime
from dateutil import tz
from pyspark.sql import functions as F
from pyspark.sql.functions import max, min, avg, count, mean, mode, lit, countDistinct, median, when, isnull, DataFrame, concat_ws, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType, DecimalType

# COMMAND ----------

#info write out
#vars and methods could be assigned to list or class
out_catalog_name ='utility'
out_database_name = 'data_profile'
out_table_name = 'fact_column_summary'
out_write_out_location = abfss_out

#Log
log_write_out_location = abfss_log_out
log_table_name = 'dataprofile_log'

#info read in 
#default values
ref_catalog = 'fivetran'
ref_database = 'salesforce'
ref_table = 'account'
ref_location = '.'.join([ref_catalog,ref_database,ref_table])

ddl_new_table = """
CREATE TABLE IF NOT EXISTS {} (    
     fact_column_summary_key        BIGINT GENERATED BY DEFAULT AS IDENTITY
    ,dim_database_key       STRING NOT NULL
    ,dim_catalog_key        STRING NOT NULL
    ,dim_column_key         STRING NOT NULL
    ,dim_table_key          STRING NOT NULL
    ,count_not_null			BIGINT 	
    ,count_distinct			BIGINT	
    ,count_isnull_isempty   BIGINT
    ,average_value          DECIMAL(18,4) 	
    ,median_value           DECIMAL(18,4) 	
    ,mode_value             DECIMAL(18,4) 	
    ,minimum_value          DECIMAL(18,4) 	
    ,maximum_value          DECIMAL(18,4) 	
    ,timestamp              TIMESTAMP 		
)
USING DELTA
LOCATION '{}'
""".format(out_table_name,out_write_out_location)

#log_table
ddl_log_table = """CREATE TABLE IF NOT EXISTS {} (
    id                BIGINT GENERATED BY DEFAULT AS IDENTITY
    ,catalog_name     STRING
    ,database_name    STRING
    ,table_name       STRING
    ,message          STRING
    ,event_type       STRING
    ,event_time       TIMESTAMP
) USING DELTA
LOCATION '{}'""".format(log_table_name, log_write_out_location)

log_schema = StructType([
    StructField('catalog_name', StringType(), False)
    ,StructField('database_name', StringType(), False)
    ,StructField('table_name', StringType(), False)
    ,StructField('message', StringType(), False)
    ,StructField('event_type', StringType(), True)
    ,StructField('event_time',TimestampType(),False)    
    ])

out_schema = StructType([    
    StructField('dim_database_key', StringType(), False)      
    ,StructField('dim_catalog_key',        StringType(), False)
    ,StructField('dim_column_key',         StringType(), False)
    ,StructField('dim_table_key' ,        StringType(), False)
    ,StructField('count_not_null',		   LongType(), True) 	
    ,StructField('count_distinct',		   LongType(), True) 	
    ,StructField('count_isnull_isempty',   LongType(), True) 	
    ,StructField('average_value',          DecimalType(18,4), True)
    ,StructField('mode_value',             DecimalType(18,4), True)	
    ,StructField('minimum_value',          DecimalType(18,4), True)	
    ,StructField('maximum_value',          DecimalType(18,4), True) 	
    ,StructField('timestamp',              TimestampType(), True) 		
    ])


# COMMAND ----------

#update  variables.
def SetRefLocation(catalog:str, database:str, table:str) -> None:
    global ref_catalog 
    ref_catalog = catalog
    
    global ref_database 
    ref_database = database
    
    global ref_table 
    ref_table = table
    
    global ref_location
    ref_location = '.'.join([ref_catalog,ref_database,ref_table])

def ReadDeltaTable(location:str) -> DataFrame:
    
    return spark.read.format('delta').table(location)
def ReturnTableDataFrame(table_name:str) -> DataFrame:
    
    SetRefLocation(ref_catalog,ref_database,table_name)   
    # convert to universal time
    #cTimestamp = datetime.now().replace(tzinfo=tz.tzutc())
    print("Starting to process table {} at {}".format(ref_location,str(datetime.now())))  
    
        
    return ReadDeltaTable(ref_location)

def WriteOutProfiledFieldInformationToDelta(df:DataFrame) -> None:
    
    df.write.format('delta').mode('append').option("mergeSchema", "true").saveAsTable(out_table_name)

#simple pipe delim string for key. hashing not defined. dbt surragate_key uses md5. prefer SHA256 over large datasets to avoid collisions. Need to define how to handle spaces and emptys for hash.

def GenerateRecordKey(fields) -> str:
    return '|'.join(fields)

def GenerateSQLCommandByDataType(datatype:str, fieldName:str, databaseName:str, catalog:str, table:str) -> str:    
    
    selectStatement = "Empty"   
    #column_name
    if datatype == 'boolean' or datatype == 'string':
        selectStatement = [
             lit(GenerateRecordKey([catalog,databaseName])).cast('string').alias('dim_database_key')\
           , lit(GenerateRecordKey([catalog])).cast('string').alias('dim_catalog_key')\
           , lit(GenerateRecordKey([catalog,databaseName,table,fieldName])).cast('string').alias('dim_column_key')\
           , lit(GenerateRecordKey([catalog,databaseName,table])).cast('string').alias('dim_table_key')\
           , count(fieldName).cast('bigint').alias('count_not_null')\
           , countDistinct(fieldName).cast('bigint').alias('count_distinct')\
           , count(when(isnull(fieldName), fieldName).when( F.trim(fieldName) == "", fieldName)).cast('bigint').alias('count_isnull_isempty')\
           , lit('0').cast('decimal(18,4)').alias('average_value')\
           , lit('0').cast('decimal(18,4)').alias('mode_value')\
           , lit('0').cast('decimal(18,4)').alias('minimum_value')\
           , lit('0').cast('decimal(18,4)').alias('maximum_value')\
           , F.current_timestamp().alias('timestamp')\
            ]    
    
    elif datatype == 'date' or datatype == 'timestamp':
        selectStatement = [
             lit(GenerateRecordKey([catalog,databaseName])).cast('string').alias('dim_database_key')\
           , lit(GenerateRecordKey([catalog])).cast('string').alias('dim_catalog_key')\
           , lit(GenerateRecordKey([catalog,databaseName,table,fieldName])).cast('string').alias('dim_column_key')\
           , lit(GenerateRecordKey([catalog,databaseName,table])).cast('string').alias('dim_table_key')\
           , count(fieldName).cast('bigint').alias('count_not_null')\
           , countDistinct(fieldName).cast('bigint').alias('count_distinct')\
           , count(when(isnull(fieldName), fieldName).when( F.trim(fieldName) == "", fieldName)).cast('bigint').alias('count_isnull_isempty')\
           , lit('0').cast('decimal(18,4)').alias('average_value')\
           , mode(fieldName).cast('decimal(18,4)').alias('mode_value')\
           , min(fieldName).cast('decimal(18,4)').alias('minimum_value')\
           , max(fieldName).cast('decimal(18,4)').alias('maximum_value')\
           , F.current_timestamp().alias('timestamp')\
        ]
    else:
        selectStatement = [
             lit(GenerateRecordKey([catalog,databaseName])).cast('string').alias('dim_database_key')\
           , lit(GenerateRecordKey([catalog])).cast('string').alias('dim_catalog_key')\
           , lit(GenerateRecordKey([catalog,databaseName,table,fieldName])).cast('string').alias('dim_column_key')\
           , lit(GenerateRecordKey([catalog,databaseName,table])).cast('string').alias('dim_table_key')\
           , count(fieldName).cast('bigint').alias('count_not_null')\
           , countDistinct(fieldName).cast('bigint').alias('count_distinct')\
           , count(when(isnull(fieldName), fieldName).when( F.trim(fieldName) == "", fieldName)).cast('bigint').alias('count_isnull_isempty')\
           , avg(fieldName).cast('decimal(18,4)').alias('average_value')\
           , mode(fieldName).cast('decimal(18,4)').alias('mode_value')\
           , min(fieldName).cast('decimal(18,4)').alias('minimum_value')\
           , max(fieldName).cast('decimal(18,4)').alias('maximum_value')\
           , F.current_timestamp().alias('timestamp')\
        ]                    
           
    return selectStatement
    
def GetMaxDateOfLastRecord(catalog:str,database:str,column:str) -> datetime:   
    maxDate = spark.sql("SELECT max(timestamp)\
    FROM " + ref_out_database + "\
    WHERE column_name =  '" + column + "' AND catalog_name = '" + catalog + "' AND column_name = '" + column + "'" ).collect()[0][0]
    return maxDate

def WriteToLog(message:str, event_type:str) -> None:
    
    schema = StructType([ \
    StructField("catalog_name",StringType(),False), \
    StructField("database_name",StringType(),False), \
    StructField("table_name",StringType(),False), \
    StructField("message",StringType(),False), \
    StructField("event_type",StringType(),False), \
    StructField("event_time",TimestampType(),True)
    ])
    data = [(ref_catalog, ref_database, ref_table, message,event_type, datetime.now())]
    df = spark.createDataFrame(data=data,schema=schema)
    df.write.format('delta').mode('append').saveAsTable(log_table_name)

# COMMAND ----------

#CREATE TABLE
spark.sql("USE CATALOG {}".format(out_catalog_name))
spark.sql("USE DATABASE {}".format(out_database_name))
spark.sql(ddl_new_table)
spark.sql(ddl_log_table)

# COMMAND ----------

## Get list of tables in fivertran.salesforce schema
list_tables = spark.sql("SELECT table_name FROM {}.information_schema.tables WHERE table_schema = '{}'".format(ref_catalog,ref_database)).collect()


# COMMAND ----------

for tbl in list_tables:  
    df_dut = ReturnTableDataFrame(tbl[0])
    #df_dut.count() # when it gets to approx 400 or more columns in a table. Spark gets stuck creating plan. For example salesforce.leads took 1-hour.
    #df_dut.cache()

    column_count = len(df_dut.columns)
    print('{} columns for {}'.format(column_count,tbl[0]) )

    #if length > 200 split 
    if column_count >= 300:
        column_length_limit = 200
        iCount, iCountUpper = 0,0
        #zero indexed array

        while iCount <= column_count:
            if (iCount + column_length_limit) < column_count:
                iCountUpper += column_length_limit
            elif iCount == column_count:
                iCountUpper += 1
            else:
                iCountUpper += (column_count - iCountUpper)

            df_limited = df_dut.select([df_dut.columns[i] for i in range(iCount,iCountUpper)])
            df_limited.count()
            df_limited.cache()

            table_column_summary2 = []
            ## Loop thru fields in table and write descriptive information to file
            fields2 = df_limited.columns
            for field1 in fields2:

                datatype2 = [dtype for name, dtype in df_limited.dtypes if name == field1][0]        
                #print("field {} with datatype {} at {}".format(field1,datatype2,datetime.now()))
                selectStatement2 = GenerateSQLCommandByDataType(datatype=datatype2,catalog=ref_catalog,databaseName=ref_database,fieldName=field1,table=ref_table)
                df31 = df_limited.select(selectStatement2)
                df31.count()
                #WriteToLog('Starting to process column {} datatype: {}'.format(field, datatype),'column')                

                table_column_summary2.append(df31.collect()[0])                   
                #df_record.show()                   

            print('Finished processing {} to {} write to table start {}'.format(iCount, iCountUpper, datetime.now()))
            df_write_out2 = spark.createDataFrame(data=table_column_summary2,schema=out_schema)           
            df_write_out2.write.mode("append").option("mergeSchema", "true").saveAsTable(out_table_name)     
            print('End write to table {}'.format(datetime.now()))
            df_limited.unpersist()

            iCount += column_length_limit

    else:
        df_dut.count()
        df_dut.cache()
        fields = df_dut.columns
        
        table_column_summary = []
        for field in fields:

            datatype = [dtype for name, dtype in df_dut.dtypes if name == field][0]        
            #print("field {} with datatype {} at {}".format(field,datatype,datetime.now()))
            selectStatement = GenerateSQLCommandByDataType(datatype=datatype,catalog=ref_catalog,databaseName=ref_database,fieldName=field,table=ref_table)
            df3 = df_dut.select(selectStatement)
            df3.count()
            #WriteToLog('Starting to process column {} datatype: {}'.format(field, datatype),'column')                

            table_column_summary.append(df3.collect()[0])                   
            #df_record.show()                   

        print('Finished processing write to table start {}'.format(datetime.now()))
        df_write_out = spark.createDataFrame(data=table_column_summary,schema=out_schema)

        df_write_out.write.mode("append").option("mergeSchema", "true").saveAsTable(out_table_name)     
        print('End write to table start {}'.format(datetime.now()))
        df_dut.unpersist()



# COMMAND ----------


